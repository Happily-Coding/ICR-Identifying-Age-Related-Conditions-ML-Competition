{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This file contains a collection of potentially useful methods, mostly from discarded approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def dirtify_prediction(prediction_request):\n",
    "    \"\"\"Adds poorly prepared data to the requested prediction, to make sure your model fail if it can't handle poorly prepared data.\"\"\"\n",
    "    prediction_request['EJ'][0]= pd.NA\n",
    "    prediction_request['EJ'][1] = 'C'\n",
    "    return prediction_request\n",
    "#prediction_request = dirtify_prediction(prediction_request)\n",
    "\n",
    "def replace_column_values(dataframe, name_of_columns_to_replace=[], replacement_dictionary={}):\n",
    "    \"\"\"Replaces values acording to the replacemente dictionary in multiple columns. of the dataframe\"\"\"\n",
    "    for column_name in name_of_columns_to_replace:\n",
    "        dataframe[column_name] = dataframe[column_name].replace(replacement_dictionary)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competition_log_loss(y_true, y_pred):\n",
    "    '''Class weighted log loss'''\n",
    "    N_0 = np.sum(1 - y_true)\n",
    "    N_1 = np.sum(y_true)\n",
    "\n",
    "    p_1 = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    p_0 = 1 - p_1\n",
    "\n",
    "    log_loss_0 = -np.sum((1 - y_true) * np.log(p_0)) / N_0\n",
    "    log_loss_1 = -np.sum(y_true * np.log(p_1)) / N_1\n",
    "\n",
    "    return (log_loss_0 + log_loss_1)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_history_for_lr_choosing():\n",
    "    \"\"\"A method to create the history for a deep learning model that can be then used to plot the learning rate graph and choose the best LR\"\"\"\n",
    "    #The technique for selecting the learning rate is covered in the [course](https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction).\n",
    "    #This is a copy pasted version of another notebook that did it manually but it should be parametrized instead. first separating the method to create a model.\n",
    "    histories = []\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(train.iloc[:,:-1], train.iloc[:,-1])):  \n",
    "        ds_train, ds_valid, valid_x, valid_y, input_shape, preprocess_pipeline = pre_processing(\n",
    "            train.iloc[train_index],train.iloc[valid_index], df)\n",
    "\n",
    "        model = Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            layers.Lambda(lambda x: tf.expand_dims(x, axis=-1)),\n",
    "\n",
    "            layers.Conv1D(16,5,padding=\"same\",\n",
    "                      kernel_initializer='lecun_normal',\n",
    "                      activation='selu'),\n",
    "            layers.Conv1D(32,2,padding=\"same\",\n",
    "                      kernel_initializer='lecun_normal',\n",
    "                      activation='selu'),\n",
    "            layers.MaxPool1D(),\n",
    "\n",
    "            layers.Flatten(),\n",
    "            layers.AlphaDropout(0.5),\n",
    "            layers.Dense(128,\n",
    "                     kernel_initializer='lecun_normal',\n",
    "                     activation=\"selu\"),\n",
    "            layers.Dense(1,activation=\"sigmoid\")\n",
    "        ])\n",
    "\n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "            lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            loss= BalancedLogLoss(),\n",
    "            metrics=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            ds_train,\n",
    "            validation_data=ds_valid,\n",
    "            epochs=150,\n",
    "            callbacks=[lr_schedule],\n",
    "            verbose=0\n",
    "        )\n",
    "        test_x = preprocess_pipeline.transform(test.iloc[:,:-1])\n",
    "        test_y = test.iloc[:,-1].to_numpy().astype(\"float32\")\n",
    "        pred = model(test_x)\n",
    "        loss = model.loss(test_y,pred)\n",
    "        histories.append(history)\n",
    "        \n",
    "def plot_lr(history:dict):\n",
    "    #Define the learning rate array\n",
    "    lrs = 1e-8 * (10 ** (np.arange(150) / 20))\n",
    "\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Set the grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot the loss in log scale\n",
    "    plt.semilogx(lrs, history.history[\"loss\"],label=\"loss\")\n",
    "    plt.semilogx(lrs, history.history[\"val_loss\"],label=\"val_loss\")\n",
    "\n",
    "    # Increase the tickmarks size\n",
    "    plt.tick_params('both', length=10, width=1, which='both')\n",
    "    plt.legend()\n",
    "    # Set the plot boundaries\n",
    "    plt.axis([1e-8, 1e-1, 0, 3])\n",
    "\n",
    "#plot_lr(histories[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_calibration_plot(y_true,\n",
    "                                    y_pred,\n",
    "                                    y_cali=None,\n",
    "                                    n_bins=30,\n",
    "                                    yerr_c=0.4,\n",
    "                                    xylim=1,\n",
    "                                    tick=0.1,\n",
    "                                    calib_method=''):\n",
    "    '''Makes a plot to show if the model probabilities are calibrated\n",
    "        It helps aid the decision of wether to calibrate it or not.\n",
    "        Calibration of uncalibrated models is important when the predicted probabilities are important.\n",
    "        Extracted from: https://www.kaggle.com/code/sergiosaharovskiy/icr-iarc-2023-eda-and-submission?scriptVersionId=129914021&cellId=47\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "            '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5), dpi=120)\n",
    "    ax = ax.flatten()\n",
    "    ax[0].errorbar(x=prob_pred, y=prob_true, yerr=abs(prob_true - prob_pred) * yerr_c, fmt=\".k\", label='Actual',\n",
    "                    color=palette[1], capthick=0.5, capsize=3, elinewidth=0.7, ecolor=palette[1])\n",
    "\n",
    "    sns.lineplot(x=np.linspace(0, xylim, 11), y=np.linspace(0, xylim, 11), color=palette[-3],\n",
    "                    label='Perfectly calibrated', ax=ax[0], linestyle='dashed')\n",
    "\n",
    "    if isinstance(y_cali, np.ndarray):\n",
    "        prob_true_, prob_pred_ = calibration_curve(y_true, y_cali, n_bins=n_bins)\n",
    "        sns.lineplot(x=prob_pred_, y=prob_true_, color=palette[-5],\n",
    "                        label=f'{calib_method} Calibration', ax=ax[0], linestyle='solid')\n",
    "\n",
    "    sns.histplot(y_pred, bins=n_bins*5, color=palette[1], ax=ax[1])\n",
    "    for i, _ in enumerate(ax):\n",
    "        ax[i].spines['top'].set_visible(False)\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "        ax[i].xaxis.grid(False)\n",
    "        ax[i].yaxis.grid(True)\n",
    "\n",
    "    ax[0].set_title(f'Probability calibration plot', fontdict={'fontweight': 'bold'})\n",
    "    ax[1].set_title(f'Histogram of predictions', fontdict={'fontweight': 'bold'})\n",
    "\n",
    "    ax[0].set_xticks(list(np.arange(0, xylim + tick, tick)))\n",
    "    ax[0].set_yticks(list(np.arange(0, xylim + tick, tick)))\n",
    "    ax[0].set(xlabel='predicted', ylabel='actual')\n",
    "    fig.suptitle(f'Predictions in range {(0, xylim)}', ha='center',  fontweight='bold', fontsize=16)\n",
    "    plt.tight_layout();\n",
    "    \n",
    "def create_logistic_calibrator(y_truths, y_predictions):\n",
    "    '''Create a logistic calbirator that can be used to fit. Requires the input to be reshaped to -1, 1'''\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    formated_test_y_predictions = y_predictions.reshape(-1, 1) ##Hace que sea un array de arrays de un elemento en vez de un array de numeros\n",
    "    lr = LogisticRegression(C=99999999999, solver='liblinear', max_iter=1000)\n",
    "    lr.fit(formated_test_y_predictions, y_truths)\n",
    "    return lr\n",
    "\n",
    "def create_isotonic_calibrator(y_truths, y_predictions):\n",
    "    from sklearn.isotonic import IsotonicRegression\n",
    "    isotonic_calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "    isotonic_calibrator.fit(y_predictions, y_truths) #Hace que sea un array de arrays de un elemento en vez de un array de numeros\n",
    "    return isotonic_calibrator\n",
    "\n",
    "#Idea to use calibrators\n",
    "#truth = model_dicts[0]['test_y_truth'] #truth es numpy\n",
    "#preds = model_dicts[0]['test_y_pred']\n",
    "#model = model_dicts[0]['model']\n",
    "\n",
    "#iso_calibrator = create_isotonic_calibrator(truth, preds)\n",
    "#log_calibrator = create_logistic_calibrator(truth, preds.numpy())\n",
    "\n",
    "#iso_cal_preds = iso_calibrator.predict(preds)\n",
    "#log_cal_preds = log_calibrator.predict(preds.numpy())\n",
    "\n",
    "#from sklearn.calibration import CalibrationDisplay\n",
    "#CalibrationDisplay.from_predictions(truth,preds)\n",
    "#CalibrationDisplay.from_predictions(truth,iso_cal_preds)\n",
    "#CalibrationDisplay.from_predictions(truth,log_cal_preds)\n",
    "#print('uncalibrated', competition_log_loss(truth.astype(\"float32\"), preds.numpy().flatten()))\n",
    "#print('iso calibrated', competition_log_loss(truth, iso_cal_preds))\n",
    "#print('logistic calibrated', competition_log_loss(truth, log_cal_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting training history and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history:list, start:int=None, end:int=None) -> None:\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    axs[0].plot(history.history[\"loss\"][start:end],label=\"loss\")\n",
    "    axs[0].plot(history.history[\"val_loss\"][start:end],label=\"val_loss\")\n",
    "    axs[0].set_xlabel(\"Epochs\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history.history[\"accuracy\"][start:end],label=\"accuracy\")\n",
    "    axs[1].plot(history.history[\"val_accuracy\"][start:end],label=\"val_accuracy\")\n",
    "    axs[1].set_xlabel(\"Epochs\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "#for model_dict in model_dicts:\n",
    "#    print('loss(test_y,model(test_x))', model_dict['loss(test_y,model(test_x))'])\n",
    "#    print('loss(test_y,model(test_df))', model_dict['loss(test_y,model(test_df))'])\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test spliting with sklearn stratified kfold\n",
    "#I wonder how it works exactly.\n",
    "#from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "#train,test = train_test_split(\n",
    "#            train_df,\n",
    "#            train_size=0.97,\n",
    "#            random_state=1,\n",
    "#            stratify=train_df.iloc[:,-1]\n",
    "#    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
